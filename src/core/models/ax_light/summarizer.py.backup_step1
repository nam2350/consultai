"""
A.X-4.0-Light MLM summarizer for efficient Korean consultation analysis.
- 7B parameters optimized for speed and Korean understanding
- 33% fewer tokens than GPT-4o for Korean inputs
- SKT's enterprise-grade Korean language processing
"""

import os
import time
import logging
import re
from typing import Dict, Any, Tuple, List

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from ..summary_utils import normalize_summary

# Suppress warnings
import warnings
warnings.filterwarnings("ignore", message=".*pad_token_id.*eos_token_id.*")

# RTX 5080 optimization
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True

logger = logging.getLogger(__name__)


class AXLightSummarizer:
    """Efficient Korean consultation summarizer using A.X-4.0-Light (LLM tier)."""

    def __init__(self, model_path: str = r"models\A.X-4.0-Light"):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        # 안전한 디바이스 감지
        try:
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                self.device = "cuda"
            else:
                self.device = "cpu"
        except Exception:
            self.device = "cpu"

        # A.X-4.0-Light optimized parameters (deterministic, speed focused)
        self.min_generation_tokens = 120
        self.max_generation_tokens = 200
        self.max_conversation_chars = 8500
        self.head_chars = 6200
        self.tail_chars = 2100
        self.generation_params = {
            'max_new_tokens': self.max_generation_tokens,
            'min_new_tokens': self.min_generation_tokens,
            'do_sample': False,
            'repetition_penalty': 1.05,
            'pad_token_id': None,
            'eos_token_id': None,
            'use_cache': True
        }

        self.min_summary_chars = 170

    def _determine_max_tokens(self, conversation_text: str) -> int:
        """Determine dynamic max_new_tokens for A.X-4.0-Light."""
        length = len(conversation_text or "")
        bonus = min(80, max(0, length // 150))
        return min(self.min_generation_tokens + bonus, self.max_generation_tokens)

    def _truncate_conversation(self, conversation_text: str) -> str:
        if not conversation_text:
            return ""
        if len(conversation_text) <= self.max_conversation_chars:
            return conversation_text
        head = conversation_text[: self.head_chars].rstrip()
        tail = conversation_text[-self.tail_chars :].lstrip()
        return head + "\n...\n" + tail

    def load_model(self) -> bool:
        """Load A.X-4.0-Light model with SKT optimizations."""
        try:
            start_time = time.time()
            logger.info(f"Loading A.X-4.0-Light from {self.model_path}")

            # Load tokenizer (requires transformers>=4.46.0)
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                local_files_only=True
            )

            # Pad token setup
            if self.tokenizer.pad_token is None:
                if self.tokenizer.eos_token:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                else:
                    self.tokenizer.pad_token = "<|endoftext|>"

            # Model loading with safer device handling
            try:
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    model_dtype = torch.bfloat16  # SKT specification for optimal performance
                    device_map = "auto"
                    max_memory = {"0": "12GB"}    # Conservative for 7B model
                else:
                    model_dtype = torch.float32
                    device_map = None
                    max_memory = None
            except Exception:
                # GPU 설정 실패시 CPU로 폴백
                model_dtype = torch.float32
                device_map = None
                max_memory = None
                self.device = "cpu"

            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
                local_files_only=True,
                device_map="auto" if self.device == "cuda" else None
            )

            # Ensure evaluation mode for inference
            self.model.eval()

            if hasattr(self.model, 'generation_config') and self.model.generation_config:
                gen_conf = self.model.generation_config
                gen_conf.do_sample = False
                gen_conf.temperature = None
                gen_conf.top_p = None
                gen_conf.top_k = None
                gen_conf.early_stopping = False

            # Set generation parameters
            self.generation_params['pad_token_id'] = self.tokenizer.pad_token_id
            self.generation_params['eos_token_id'] = self.tokenizer.eos_token_id

            load_time = time.time() - start_time
            logger.info(f"A.X-4.0-Light loaded successfully in {load_time:.2f}s")
            return True

        except Exception as e:
            logger.error(f"Failed to load A.X-4.0-Light: {str(e)}")
            return False

    def build_efficient_korean_prompt(self, conversation_text: str) -> str:
        """Build token-efficient Korean prompt optimized for A.X's efficiency."""
        # Streamlined prompt for Korean token efficiency (33% fewer tokens)
        system_prompt = (
            "당신은 콜센터 상담 대화를 요약하는 전문 AI입니다.\\n\\n"
            "입력 대화 분석:\\n"
            "- 상담사와 고객의 화자 분리가 적용된 실제 상담 대화입니다\\n"
            "- 화자 표시 오류가 있을 수 있으므로 전체 맥락으로 판단하세요\\n"
            "- 대화 흐름을 파악하여 올바른 화자를 추론하고 요약하세요\\n\\n"
            "요약 핵심 원칙:\\n"
            "1. 원본 대화에서 실제 언급된 내용만 요약\\n"
            "2. 핵심 용어, 시스템명, 절차명, 기관명은 정확히 기재\\n"
            "3. 고객의 질문과 상담사의 답변을 명확히 구분\\n"
            "4. 추측, 가정, 상상은 절대 포함 금지\\n\\n"
            "5. 각 섹션은 최소 2문장으로 작성하고 처리 상태와 근거, 후속 조치를 함께 명시\\n\\n"
            "출력 형식 (반드시 3줄 구조로 작성):\\n"
            "**고객**: 고객의 핵심 질문이나 요청사항\\n"
            "**상담사**: 상담사의 핵심 안내나 해결방안\\n"
            "**상담결과**: 상담의 최종 결과나 처리상태\\n\\n"
            "각 줄은 완전한 문장으로 작성하며, 위 형식을 정확히 준수하세요."

            f"상담:\\n{conversation_text}\\n\\n"
            
        )

        return system_prompt

    def generate_summary_fast(self, conversation_text: str) -> Tuple[str, float]:
        """Generate efficient Korean summary using A.X-4.0-Light."""
        if not self.model or not self.tokenizer:
            raise RuntimeError("Model not loaded. Call load_model() first.")

        # Prevent fake info generation for empty conversations
        if not conversation_text or len(conversation_text.strip()) < 10:
            raise RuntimeError("대화 내용이 없어서 요약할 수 없습니다")

        start_time = time.time()

        try:
            # Build efficient Korean prompt
            truncated = self._truncate_conversation(conversation_text)
            prompt = self.build_efficient_korean_prompt(truncated)

            # Tokenize with 16K context support
            model_inputs = self.tokenizer(
                [prompt],
                return_tensors="pt",
                return_attention_mask=True,
                return_token_type_ids=False,
                truncation=True,
                max_length=16384  # A.X-4.0-Light context limit
            ).to(self.device)
            model_inputs.pop('token_type_ids', None)

            # Check token efficiency
            token_length = model_inputs.input_ids.shape[1]
            if token_length > 14000:  # Approaching 16K limit
                logger.warning(f"High token usage: {token_length} tokens (conversation: {len(conversation_text)} chars)")

            generation_kwargs = dict(self.generation_params)
            dynamic_max = self._determine_max_tokens(truncated)
            generation_kwargs['max_new_tokens'] = dynamic_max
            generation_kwargs['min_new_tokens'] = max(self.min_generation_tokens, min(dynamic_max, self.min_generation_tokens + 40))
            if 'attention_mask' in model_inputs:
                generation_kwargs['attention_mask'] = model_inputs.attention_mask

            with torch.inference_mode():
                generated_ids = self.model.generate(
                    model_inputs.input_ids,
                    **generation_kwargs
                )

            # Decode response
            response_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
            response = self.tokenizer.decode(
                response_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )

            # Post-process for efficient Korean quality
            summary = self.post_process_efficient_summary(response, truncated)

            processing_time = time.time() - start_time
            return summary, processing_time

        except torch.cuda.OutOfMemoryError as e:
            processing_time = time.time() - start_time
            logger.error(f"GPU memory exhausted: {e}")
            raise RuntimeError(f"GPU memory insufficient: {str(e)}")
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Generation error: {e}")
            raise RuntimeError(f"Summary generation failed: {str(e)}")

    def post_process_efficient_summary(self, raw_summary: str, source_text: str = "") -> str:
        """Post-process A.X output for efficient Korean consultation quality."""
        if not raw_summary or len(raw_summary.strip()) < 10:
            raise RuntimeError("Generated summary is empty or too short")

        summary = normalize_summary(
            raw_summary,
            cleanup_patterns=(
                "??? ??",
                "?? ??",
                "?? ??",
                "??:",
                "??:",
            ),
            min_length=70,
            logger=logger,
            fallback_text=source_text or raw_summary,
        )

        summary = self._extend_summary_if_needed(summary, source_text)

        if self._contains_fake_enterprise_info(summary, source_text):
            raise RuntimeError("Fake information detected in enterprise summary")

        return summary
    def _optimize_korean_efficiency(self, text: str) -> str:
        """Optimize Korean text for token efficiency (A.X specialty)."""
        # SKT's Korean optimization patterns
        efficiency_patterns = [
            # Reduce redundant particles
            (r'\\b(에 대해서는|에 대하여는)\\b', '에 대해'),
            (r'\\b(을 통해서|를 통해서)\\b', '을 통해'),
            (r'\\b(에 관해서는|에 관하여는)\\b', '에 관해'),
            # Streamline formal endings
            (r'\\b(하였습니다|했었습니다)\\b', '했습니다'),
            (r'\\b(드리겠습니다|해드리겠습니다)\\b', '드립니다'),
            # Efficient conjunctions
            (r'\\b(그리고 나서|그런 다음에)\\b', '그리고'),
            (r'\\b(따라서|그러므로|그런 이유로)\\b', '따라서'),
        ]

        optimized = text
        for pattern, replacement in efficiency_patterns:
            optimized = re.sub(pattern, replacement, optimized)

        return optimized.strip()

    def _contains_fake_enterprise_info(self, summary: str, source_text: str = "") -> bool:
        """Enhanced fake information detection for Korean enterprise consultation."""
        fake_patterns = [
            # Korean date patterns not in conversation
            r'\d{4}년\d{1,2}월\d{1,2}일',
            # Specific Korean enterprise locations
            '서울 중구', '강남', '분당', '성북', '서초동',
            # Weather/incident events
            '태풍', '지진', '폭우', '침수', '정전', '서비스 장애',
            # Generic enterprise language
            '본사 정보', '관리부', '총무과',
            # Fabricated SKT services (unless mentioned)
            'SKT 고객센터', 'T멤버십', 'T메뉴',
        ]


        reference = source_text or ""

        for pattern in fake_patterns:
            if re.search(pattern, summary):
                if reference and re.search(pattern, reference):
                    continue
                logger.warning(f"[AX_FAKE_INFO] Detected enterprise pattern: {pattern}")
                return True
        return False
    def _extend_summary_if_needed(self, summary: str, source_text: str) -> str:
        if not source_text or len(summary) >= self.min_summary_chars:
            return summary

        sections = summary.split('\n')
        if len(sections) < 3:
            return summary

        result_idx = next((idx for idx, line in enumerate(sections) if line.startswith('**상담결과**')), None)
        if result_idx is None:
            return summary

        candidates = self._extract_context_sentences(source_text)
        additions: List[str] = []
        for sentence in candidates:
            if sentence in summary:
                continue
            additions.append(sentence)
            if len(' '.join(additions)) >= 80:
                break

        if additions:
            result_line = sections[result_idx].rstrip()
            if result_line and not result_line.endswith(('다', '.', '!', '?', '요')):
                result_line = result_line.rstrip() + '.'
            result_line = (result_line + ' ' + ' '.join(additions)).strip()
            sections[result_idx] = result_line
            updated = '\n'.join(sections)
            if len(updated) > len(summary):
                summary = updated
        return summary

    def _extract_context_sentences(self, source_text: str) -> List[str]:
        cleaned = re.sub(r'\.\.\.', ' ', source_text)
        cleaned = re.sub(r'\[[^\]]+\]', ' ', cleaned)
        sentences: List[str] = []
        for chunk in re.split(r'[\r\n]+', cleaned):
            chunk = chunk.strip()
            if not chunk:
                continue
            chunk = re.sub(r'^(고객|상담사|Agent|상담원)[:\s]+', '', chunk)
            for sentence in re.split(r'(?<=[.!?])\s+', chunk):
                stripped = sentence.strip()
                if len(stripped) < 12:
                    continue
                if stripped in sentences:
                    continue
                sentences.append(stripped)
                if len(sentences) >= 6:
                    return sentences
        return sentences


    def summarize_consultation(self, conversation_text: str, max_length: int = 280) -> Dict[str, Any]:
        """
        Efficient Korean consultation summarization using A.X-4.0-Light.

        Args:
            conversation_text: Consultation conversation text
            max_length: Maximum summary length (compatibility parameter)

        Returns:
            Dict with success, summary, processing_time, model_used, error
        """
        start_time = time.time()

        try:
            # Ensure model is loaded
            if not self.model or not self.tokenizer:
                if not self.load_model():
                    raise RuntimeError("Failed to load A.X-4.0-Light model")

            # Prevent fake info generation for empty conversations
            if not conversation_text or len(conversation_text.strip()) < 10:
                return {
                    'success': False,
                    'summary': '',
                    'processing_time': time.time() - start_time,
                    'model_used': 'A.X-4.0-Light',
                    'error': '대화 내용이 없어서 요약할 수 없습니다'
                }

            # Generate summary
            summary, proc_time = self.generate_summary_fast(conversation_text)

            total_time = time.time() - start_time

            return {
                'success': True,
                'summary': summary,
                'processing_time': total_time,
                'model_used': 'A.X-4.0-Light',
                'error': ''
            }

        except Exception as e:
            total_time = time.time() - start_time
            error_msg = str(e)
            logger.error(f"A.X-4.0-Light summarization failed: {error_msg}")

            return {
                'success': False,
                'summary': '',
                'processing_time': total_time,
                'model_used': 'A.X-4.0-Light',
                'error': error_msg
            }

    def cleanup(self):
        """Optimized memory cleanup for A.X-4.0-Light."""
        try:
            if self.model:
                if hasattr(self.model, 'cpu'):
                    self.model.cpu()
                del self.model
                self.model = None

            if self.tokenizer:
                del self.tokenizer
                self.tokenizer = None

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            import gc
            gc.collect()

        except Exception as e:
            logger.warning(f"Cleanup warning: {e}")


# Standalone function for compatibility
def summarize_with_ax_light(conversation_text: str, model_path: str = r"models\A.X-4.0-Light") -> Dict[str, Any]:
    """
    Efficient Korean summarization with A.X-4.0-Light (LLM tier).

    Args:
        conversation_text: Conversation to summarize
        model_path: Path to A.X-4.0-Light model

    Returns:
        Dict with success, summary, error, processing_time, model_name
    """
    start_time = time.time()
    summarizer = None

    try:
        # Initialize and load model
        summarizer = AXLightSummarizer(model_path)

        if not summarizer.load_model():
            raise RuntimeError("Failed to load A.X-4.0-Light model")

        # Generate summary
        summary, proc_time = summarizer.generate_summary_fast(conversation_text)

        return {
            'success': True,
            'summary': summary,
            'processing_time': proc_time,
            'model_name': 'A.X-4.0-Light',
            'error': ''
        }

    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = str(e)
        logger.error(f"A.X-4.0-Light standalone summarization failed: {error_msg}")

        return {
            'success': False,
            'summary': '',
            'error': error_msg,
            'processing_time': processing_time,
            'model_name': 'A.X-4.0-Light'
        }

    finally:
        if summarizer:
            summarizer.cleanup()


if __name__ == "__main__":
    print("[ERROR] A.X-4.0-Light 분석기는 직접 실행하지 마세요")
    print("[INFO] MLM 배치 처리 시스템에서 호출됩니다")